{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer3",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/2Jeo1JL+zKmOGV5qKaDo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucia-KIM/with-flower/blob/main/transformer3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xflLnitysVc-",
        "outputId": "eaaa03c6-3b32-4989-99c1-7917dc2606a9"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "torch.__version__"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.1+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7eOE-wUoDET"
      },
      "source": [
        "## 트랜스포머 아키텍쳐 구현\n",
        "\n",
        "### Multi Head Attention \n",
        "\n",
        "- Scaled Dot-Product Attention 구현해서, \n",
        "- Multi-Head Attention으로\n",
        "- Scaled Dot-Product attention을 겹쳐놓은 h개의 Scaled Dot-Product attention(Multi-Head attention)을 사용\n",
        "- 각각의 Attention에는 Query, Key, Value가 사용되었는데, 이 세 값들을 바로 사용하는 것이 아니라 h개의 Attention각각에 대해서 다르게 초기화된 Parameter Matrix를 곱하여 (Projection) 사용한다. \n",
        "- 즉, 이 과정에서 h개의 다른 값들을 얻게되고 이를 concat해서 attention결과물을 얻기 때문에 하나의 Scaled Dot-Product Attention을 사용할 때보다 주어진 Query, Key, Value에 대해서 보다 다양한 상황(subspace)에 대한 attention을 계산할 수 있게 된다. \n",
        "\n",
        "#### 1) Multi-Head Attention은 쿼리, 키, 벨류가 Linear로 각각 scaled dot-product attention에 들어가서 concat되는 형식임 \n",
        "#### 2) Scaled Dot-Product Attention은 Q와 K를 행렬 곱하고, 스케일 한 다음에 마스크를 씌우고(선택) 소프트 맥스를 거친 결과를 V와 행렬곱한다. \n",
        "- Query가 들어오게 되면 Key값과의 계산을 통하여 기준 값 Key에 대한 Query의 상대적인 weight(softmax)를 계산한다.\n",
        "- 이 Weight를 Value에 곱함으로써 Query와 Key의 연관성을 기준으로 한 새로운 값을 얻게 된다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpTbvBHmFJk7"
      },
      "source": [
        "import torch.nn as nn\n",
        "# TORCH.NN: 신경망(neural network) 구조가 디자인 된 모듈과 클래스들을 제공, 필요에 따라  커스터마이즈하여 사용\n",
        "# 하이퍼 파라미터로 hidden_dim, n_heads, dropout_ratio 지정 \n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device): \n",
        "        super().__init__()   # 생성자 \n",
        "\n",
        "        assert hidden_dim % n_heads == 0  \n",
        "        # assert는 뒤의 조건이 True가 아니면 AssertError를 발생\n",
        "        # 나머지가 0인 경우를 찾는다. \n",
        "\n",
        "        self.hidden_dim = hidden_dim  # 하나의 단어에 대한 임베딩 차원\n",
        "        self.n_heads = n_heads # head의 개수 = scaled dot-product attention의 개수\n",
        "        self.head_dim = hidden_dim // n_heads # 임베딩 차원을 헤드 개수로 나눈 몫, 즉 각 헤드에서의 임베딩 차원\n",
        "\n",
        "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 fc레이어\n",
        "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key값에 적용될 fc레이어\n",
        "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 fc레이어\n",
        "\n",
        "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout_ratio) # 드롭 아웃 비율\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device) \n",
        "        # 각 헤드의 임베딩 차원의 제곱근을 구해서 to()? gpu 사양만큼 스케일한다.\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        # mask = None인 경우\n",
        "\n",
        "        batch_size = query.shape[0] # 쿼리 행렬의 행의 개수\n",
        "        # query의 모양은 [batch_size, query_len, hidden_dim] 형태, 여기서 인덱스 0번 batch_size을 받음\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        # hidden_dim -> n_heads * head_dim 형태로 변경\n",
        "        # --> 하나의 단어에 대한 임베딩 차원을 각 헤드의 임베딩 차원*어텐션의 갯수로 변경\n",
        "        # n_heads(h)개의 서로 다른 어텐션 컨셉을 학습하게 함\n",
        "\n",
        "        # view()는 tensor의 모양을 바꾸는데 사용(데이터의 구조가 변경될 뿐 순서는 변경되지 않는다)\n",
        "        # view가 반환한 tensor는 원본 tensor와 기반이 되는 data를 공유한다. 만약 반환된 tensor의 값이 변경된다면, viewed되는 tensor에서 해당하는 값이 변경된다.\n",
        "        # permute(순서 인덱스)는 모든 차원들을 맞교환할 수 있다.\n",
        "        # 지금까지 Q는 [batch_size, query_len, hidden_dim] 형태로 저장되었음. \n",
        "        # 이걸 view()를 사용해 Q: [batch_size, n_heads, query_len, head_dim]로 텐서의 모양을 바꿔줌 \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "        \n",
        "        # (Attention Energy) Scaled Dot-Product Attention을 만들기 위해, \n",
        "        # input은 d_k dimension의 queries와 keys, 그리고 d_v dimension의 values로 구성된다. \n",
        "        # query와 key의 dot product를 계산하여 \\sqrt{d_k}로 나눈다.\n",
        "        # \\sqrt{d_k}로 나누어 준, 다시말해 scaled하였기 때문에 Scaled Dot-Product Attention\n",
        "        # 이후 softmax function**을 사용하여 values에 대한 weights를 얻어 낸다.\n",
        "        energy = torch.matmul(Q, K.permute(0,1,3,2))/self.scale\n",
        "        # matmul(): 3차원 이상의 행렬끼리 곱\n",
        "        # energy: [batch_size, n_heads, query_len, key_len]\n",
        "        # 여기서 Q는 [batch_size, n_heads, query_len, head_dim],\n",
        "        # K.permute(0,1,3,2)는 [batch_size, n_heads, head_dim, key_len]\n",
        "        # 그럼 head_dim* query_len = query_len이고, head_dim*key_len = key_len인가? \n",
        "\n",
        "\n",
        "        # 마스크를 사용하는 경우\n",
        "        if mask is not None:\n",
        "            # 마스크를 부분을 아주 작은 값으로 채우기 -1e10\n",
        "            energy = energy.masked_fill(mask==0, -1e10)\n",
        "\n",
        "        # 소프트맥스로 어텐션 스코어 계산 : 각 단어에 대한 확률\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        # query와 key에 대한 dot-product를 계산하면 각각의 query와 key 사이의 유사도를 구할 수 있게 된다.\n",
        "        # attention: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
        "        x = torch.matmul(self.dorpout(attention), v)\n",
        "        # key와 value는 attention이 이루어지는 위치에 상관없이 같은 값을 갖게 되는데, \n",
        "        # 여기서 x는 [batch_size, n_heads, query_len, head_dim] 이다.\n",
        "\n",
        "        x = x.permute(0,2,1,3).contiguous()\n",
        "        # x: [batch_size, query_len, n_heads, head_dim]\n",
        "        # contiguous()는 새로운 메모리에 할당하여 주소값 재배열이 가능하다. \n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hidden_dim)\n",
        "        #self.head_dim = hidden_dim // n_heads 였으니, head_dim*n_heads = hidden_dim\n",
        "        # 그래서 x는 [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        return x, attention"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX3DNb49VAm0"
      },
      "source": [
        "## Position-wise Feedforward Networks\n",
        "- 인코더와 디코더의 각각 position(개별 단어마다)에 개별적으로 동일하게 적용되는 fully connected feed-forward network sub-layerrk 있음 \n",
        "- ReLU 함수를 포함한 두개의 선형 변환(linear transformation 1, 2)으로 구성됨\n",
        "- input(x) -> linear transformation_1 -> ReLU -> linear transformation_2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJQQSwZYVAO8"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(hidden_dim, pf_dim) # linear transformation_1\n",
        "        self.fc_2 = nn.Linear(pf_dim, hidden_dim) # linear transformation_2\n",
        "        # hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
        "        # pf_dim: Feedforward 레이어에서의 내부 임베딩 차원\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout_ratio) # dropout_ratio: 드롭아웃(dropout) 비율\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        # 선형변화 1에 ReLU를 적용한 x\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "        # 이어서 선형변화 2를 적용함 \n",
        "\n",
        "        return x"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgsVwR7lacaK"
      },
      "source": [
        "## Encoder Layer\n",
        "- 하나의 인코더 레이어를 구현\n",
        "- 입력과 출력의 차원이 같다.\n",
        "- 트랜스포머는 이런 인코더 레이어를 여러번 중첩하여 사용한다. \n",
        "\n",
        "### 간단한 구조는, \n",
        "    1) 소스에 대한 어텐션 + 정규화\n",
        "    2) positionwise feedforward + 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcIRiSkkbPOJ"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)  \n",
        "        # Layer Normalization\n",
        "        # 정규화란 무언가를 표준화 시키거나 다른 것과 비교하기 쉽도록 바꾸는 것\n",
        "        # 따라서, 정규화는 데이터의 범주를 바꾸는 작업으로 스케일이 곧 정규화\n",
        "        #선형대수학에서 놈은 벡터의 크기(magnitude) 또는 길이(length)를 측정하는 방법을 의미\n",
        "\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 하나의 임베딩이 복제되어 쿼리, 키, 벨류로 입력되는 방식\n",
        "    def forward(self, src, src_mask):\n",
        "        # src(소스는) [batch_size, src_len, hidden_dim]\n",
        "        # src_mask : [batch_size, src_len]\n",
        "\n",
        "        # 인코더 self-attention layer는 \n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        # 필요한 경우 self attention에서 마크스 행렬을 이용하여 어텐션할 단어를 조절 가능\n",
        "        # src-src 이거나, src-src_masked\n",
        "        # 디코더에서는 인코더와 달리 순차적으로 결과를 만들어야 하기 때문에 mask를 사용함\n",
        "        # 다시말해 특정 단어 position i보다 뒤에 있는 position에 어텐션을 주지 못하게 함\n",
        "\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        # dropout, residual connection and layer norm\n",
        "        # 각각의 계산을 진행할 때는 Residual Connection{=LayerNorm(x + Sublayer(x))}과 Layer normalization을 적용한다.\n",
        "        # Residual Connection의 개념은 기존에 학습한 정보를 보존하고, 거기에 추가적으로 학습한 정보를 더하는 형식이다.\n",
        "        # x의 아웃풋이 y라고 할때, y=f(x)는 direct로 학습한 경우라고 생각할 수 있다. 아웃풋인 y는 x를 통해 새롭게 학습한 정보를 의미한다.\n",
        "        # y=f(x)인 경우는 기존에 학습한 정보를 보존하지 않고 변형시켜 새롭게 생성하는 정보이다.\n",
        "        # 이 경우 레이어의 깊이 깊어질수록 한번에 학습해야 할 mapping이 너무 많아져 학습이 어려워지는 문제가 발생한다.\n",
        "        # 반면에 Residual Connection은 y=f(x)+x로 이전에 학습한 내용 x를 더해주어(보존)하여 추가되는 학습만 진행하면 된다는 장점이 있다.\n",
        "        # 논문에서는 Residual Connection이 포함된 self attention의 장점으로 layer당 계산량이 줄어든다고 기재해 놓았다. \n",
        "        \n",
        "        # position-wise feedforward\n",
        "        _scr = self.positionwise_feedforward(scr)\n",
        "\n",
        "        src = self.ff_layer_norm(src+self.dropout(_src))\n",
        "        # dropout, residual and layer norm\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "        \n",
        "        return src"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTPk4qJKRbv6"
      },
      "source": [
        "## 인코더 아키텍처\r\n",
        "\r\n",
        ": 전체 인코더 아키텍처 정의\r\n",
        "\r\n",
        "- 전처리 후 토큰화 된 소스를 input embedding\r\n",
        "- positional encoding을 거쳐 encoder 박스로 들어감.\r\n",
        "    * 트랜스포머는 단어의 순서(sequence)를 이용하기 위해 position에 대한 정보를 추가해 주어야 함.\r\n",
        "    * 논문에서는 서로 다른 빈도의 사인과 코사인 함수를 이용한다. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt4hLjmuRYdQ"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        # nn.Embedding()는 임베딩 층(embedding layer)을 만들어 훈련 데이터로부터 처음부터 임베딩 벡터를 학습하는 방법\r\n",
        "        # nn.Embedding()을 사용하여 학습가능한 임베딩 테이블 만든다. \r\n",
        "        # nn.Embedding은 크게 두 가지 인자를 받는데, \r\n",
        "        # 1) num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기\r\n",
        "        # 2) embedding_dim : 임베딩 할 벡터의 차원\r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\r\n",
        "        # input_dim: 하나의 단어에 대한 원 핫 인코딩 차원\r\n",
        "        # hidden_dim: 하나의 단어에 대한 임베딩 차원\r\n",
        "\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\r\n",
        "        # max_length: 문장 내 최대 단어 개수\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\r\n",
        "        # layer의 갯수는 앞서 만들어 놓은 인코더 레이어의 수를 가져온다.\r\n",
        "        # nn.ModuleList()는 각 레이어를 리스트에 전달하고 레이어의 iterator를 만든다.\r\n",
        "        # 인코더 레이어의 수 만큼 for를 돌려서 layer의 수를 정한다.\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\r\n",
        "\r\n",
        "    def forward(self, src, src_mask):\r\n",
        "\r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        # src는 [batch_size, src_len]로 구성되어 있음\r\n",
        "\r\n",
        "        # positional encoder를 논문과 다르게 학습하는 형태로 구현\r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        # unsqueeze함수는 지정한 자리에 1인 차원을 생성하는 함수이다. \r\n",
        "        # repeat(object, n)는 object를 n번 반복함\r\n",
        "        # 여기서는 소스의 길이만큼 1차원 텐서를 만들어서, unsqueeze()를 통해 2차원으로 만들고,\r\n",
        "        # unsqueeze로 만든 1차원 자리에 배치 사이즈만큼의 차원으로 바꿔준다. \r\n",
        "        # 결국 pos는 [batch_size, src_len] 형태가 된다.\r\n",
        "\r\n",
        "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, src_mask)\r\n",
        "        # src: [batch_size, src_len, hidden_dim]\r\n",
        "\r\n",
        "        return src # 마지막 레이어의 출력을 반환"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1b1m-fRgc11"
      },
      "source": [
        "## Decoder Layer \r\n",
        ": 하나의 디코더 레이어 정의\r\n",
        "- 트랜스포머의 디코더는 디코더 레이어를 여러 번 중첩해 사용\r\n",
        "- 디코더 레이어에서는 2개의 Multi-Head Attention 레이어가 사용 됨\r\n",
        "    * 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록 만들기 위해 Masked Multi-Head Attention 사용\r\n",
        "    * 인코더에서 출력된 내용에 대한 Multi-Head Attention\r\n",
        "- 이후에 positionwise feedforward를 진행함\r\n",
        "\r\n",
        "### 전체 구조를 간단히 보면,\r\n",
        "    1) 타겟에 대한 마스크 어텐션 + 정규화\r\n",
        "    2) 인코더 출력에 대한 어텐션 + 정규화\r\n",
        "    3) positionwise feedforward + 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8K0lnXzc6oh"
      },
      "source": [
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "\r\n",
        "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조 \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\r\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\r\n",
        "        # trg_mask: [batch_size, trg_len]\r\n",
        "        # src_mask: [batch_size, src_len]\r\n",
        "\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        # 타겟 데이터 스스로 자신에게 어텐션하는 구조\r\n",
        "\r\n",
        "        trg = self_attn_layer_norm(trg+self.dropout(_trg))\r\n",
        "        # dropout, residual connection and layer norm\r\n",
        "\r\n",
        "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        # encoder_attention은 x(단어)와 attention(확률)을 반환 \r\n",
        "        \r\n",
        "        trg = self.enc_attn_layer_norm(trg+self.dropout(_trg))\r\n",
        "        # encoder에서 받은 x(_trg)와 trg로 encoder attention layer의 normalization 한다.\r\n",
        "        # dropout, residual connection and layer norm\r\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\r\n",
        "\r\n",
        "        # 타겟을 masked 어텐션한 것 1층\r\n",
        "        # 인코더의 출력 내용 어텐션 1층\r\n",
        "        # 이제 positionwise feedforward 할 차례임\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        trg = self.ff_layer_norm(trg+self.dropout(_trg))\r\n",
        "\r\n",
        "        return trg, attention\r\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\r\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwffDkQmONzE",
        "outputId": "96f072c8-6fb4-47ec-a0c1-58154262b4db"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5lkJ2_atszk"
      },
      "source": [
        "## 디코더 아키텍처\r\n",
        ": 전체 디코더 아키텍처 정의\r\n",
        "- 이번에도 논문과 다르게, 위치 임베딩(positional embedding)을 학습하는 형태로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTXEdCpTc-3H"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\r\n",
        "        # output_dim: 하나의 단어에 대한 원 핫 인코딩 차원\r\n",
        "        # hidden_dim: 하나의 단어에 대한 임베딩 차원\r\n",
        "        \r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\r\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\r\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\r\n",
        "\r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        # trg: [batch_size, trg_len]\r\n",
        "\r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        # pos: [batch_size, trg_len]\r\n",
        "\r\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\r\n",
        "        # 토큰 임베딩을 스케일해서, positional 임베딩과 더함. \r\n",
        "        # 이걸 드롭아웃 처리\r\n",
        "        # trg에 hidden_dim이 더해져서 [batch_size, trg_len, hidden_dim] 구조가 됨\r\n",
        "\r\n",
        "        # self.layers는 nn.ModuleList()로 decoder layers의 레이어를 하나씩 담은 리스트\r\n",
        "        for layer in self.layers:\r\n",
        "            # 소스 마스크와 타겟 마스크 모두 사용\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)            \r\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\r\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\r\n",
        "\r\n",
        "        output = self.fc_out(trg)\r\n",
        "        # fc_out(trg)의 trg과 fc_out()의 합성곱의 형태?? \r\n",
        "        # self.fc_out가 nn.Linear(hidden_dim, output_dim)이니\r\n",
        "        # 최종 output은 [batch_size, trg_len, output_dim] 형태\r\n",
        "        \r\n",
        "        return output, attention"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIOUd26M5d2B"
      },
      "source": [
        "## 트랜스포머(Transformer) 아키텍처\r\n",
        ": 최종 전체 트랜스포머 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ps4VldHdKxV"
      },
      "source": [
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        # 소스 문장의 <pad> 토큰(padding)에 대하여 마스크(mask) 값을 0으로 설정\r\n",
        "\r\n",
        "    def make_src_mask(self, src):\r\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        # src는 [batch_size, src_len]의 형태, 패딩 토큰을 제외하고 \r\n",
        "        # 일반 소스에는 unsqueeze() 함수로 [batch_size, 1, 1, src_len] 구조로 바꿔준다.\r\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\r\n",
        "\r\n",
        "        return src_mask\r\n",
        "\r\n",
        "    # 타겟 문장에서는 다음 단어가 무엇인지 모르게 mask사용\r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        # trg: [batch_size, trg_len]\r\n",
        "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\r\n",
        "\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n",
        "        # tril()은 행렬의 대각행렬 아래쪽을 지정한 수로 바꾸고 위쪽은 0으로 반환\r\n",
        "        # 여기서는 ones()를 통해 1로 바꿈, 아래와 같은 형식\r\n",
        "        \"\"\" (마스크 예시)\r\n",
        "        1 0 0 0 0\r\n",
        "        1 1 0 0 0\r\n",
        "        1 1 1 0 0\r\n",
        "        1 1 1 1 0\r\n",
        "        1 1 1 1 1\r\n",
        "        \"\"\"\r\n",
        "        # trg_sub_mask: [trg_len, trg_len]\r\n",
        "\r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        # &는 비트연산자로 AND 연산한다. 둘다 참일때만 만족\r\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\r\n",
        "\r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        # src: [batch_size, src_len]\r\n",
        "        # trg: [batch_size, trg_len]\r\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\r\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\r\n",
        "\r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\r\n",
        "\r\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n",
        "        # output: [batch_size, trg_len, output_dim]\r\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\r\n",
        "\r\n",
        "        return output, attention"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inXnxQ48E-GF"
      },
      "source": [
        ""
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY3-Hsg-HCz_"
      },
      "source": [
        "## 모델 학습하기 \r\n",
        "\r\n",
        "### 작업 순서 \r\n",
        "    1) 학습할 데이터 불러오기\r\n",
        "    2) 데이터 전처리\r\n",
        "    3) 모델 초기화 및 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA6RULDp3VA-",
        "outputId": "ff028007-3622-45e1-8f0b-597e32b5f963"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (0.1.95)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.7.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3tAKg__HmEI"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmBezBvwBXUk"
      },
      "source": [
        "%%capture   \r\n",
        "#출력 내용이 나오지 않도록 억제\r\n",
        "\r\n",
        "# 토큰화를 지원하는 spacy라이브러리를 사용할 예정 \r\n",
        "!python -m spacy download en\r\n",
        "!python -m spacy download de\r\n",
        "\r\n",
        "# ! 는 작업의 강제성을 부여, 관리자 권한을 줄때(sudo처럼)\r\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN0hUcVhHsLG"
      },
      "source": [
        "import spacy\r\n",
        "spacy_en = spacy.load('en')  # 영어 토큰화 기법이 담긴 객체 생성\r\n",
        "spacy_de = spacy.load('de') # 독일어 토큰화"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUDBFBJXHuxJ"
      },
      "source": [
        "# 각 언어의 토큰화 함수 정의\r\n",
        "\r\n",
        "def tokenize_de(text):\r\n",
        "    return [token.text for token in spacy_de.tokenizer(text)]\r\n",
        "\r\n",
        "def tokenize_en(text):\r\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]\r\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7yU06wcHw5_"
      },
      "source": [
        "# torchtext.data에는 필드(Field)라는 도구를 제공하는데, 필드를 통해 앞으로 어떤 전처리를 할 것인지를 정의한다.\r\n",
        "# 번역 목표 : 소스(SRC) - 독일어, 타겟(TRG) - 영어\r\n",
        "\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "\r\n",
        "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\r\n",
        "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\r\n",
        "# 초기 입력 토큰은 문자열-시작을 알리는 (start-of-string) <SOS> 토큰으로 지정\r\n",
        "# 동일하기 마지막 토큰을 알리는 <eos> (end-of-string) 지정\r\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo0i5mZxHzD_"
      },
      "source": [
        "# 파이토치의 영-독 데이터 셋을 불러온다.\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "\r\n",
        "train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts=(\".de\",\".en\"), fields=(SRC, TRG))\r\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np64cCAfH1OV",
        "outputId": "4a24b609-8818-473b-fa2d-2ed3b398c5d8"
      },
      "source": [
        "# 각 학습, 검증, 테스트 데이터로 분리된 데이터의 양 확인\r\n",
        "print(len(train_dataset.examples), len(valid_dataset.examples), len(test_dataset.examples))\r\n",
        "\r\n",
        "print(type(train_dataset.examples[30]))\r\n",
        "vars(train_dataset.examples[30])\r\n",
        "# vars([object])는 개체의 속성을 리턴해주는 함수\r\n",
        "# 다시말하면, torchtext.data.example은 Example 클래스 안에 소스 단어들을 속성으로 가지고 있다고 보면 됨.\r\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29000 1014 1000\n",
            "<class 'torchtext.data.example.Example'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'src': ['ein',\n",
              "  'mann',\n",
              "  ',',\n",
              "  'der',\n",
              "  'mit',\n",
              "  'einer',\n",
              "  'tasse',\n",
              "  'kaffee',\n",
              "  'an',\n",
              "  'einem',\n",
              "  'urinal',\n",
              "  'steht',\n",
              "  '.'],\n",
              " 'trg': ['a',\n",
              "  'man',\n",
              "  'standing',\n",
              "  'at',\n",
              "  'a',\n",
              "  'urinal',\n",
              "  'with',\n",
              "  'a',\n",
              "  'coffee',\n",
              "  'cup',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkB2sz0cH51y",
        "outputId": "fa87b61e-ed65-4919-df5d-1336f7df393f"
      },
      "source": [
        "#field의 build_vocab() 도구를 사용하면 단어 집합(사전)을 생성할 수 있음\r\n",
        "SRC.build_vocab(train_dataset, min_freq=2)\r\n",
        "TRG.build_vocab(train_dataset, min_freq=2)\r\n",
        "# min_freq : 단어 집합에 추가 시 단어의 최소 등장 빈도 조건을 추가\r\n",
        "# 여기서는 최소 두번 이상 나온 단어만 사전으로 생성\r\n",
        "\r\n",
        "print('SRC 단어 집합의 크기 : {}'.format(len(SRC.vocab)))\r\n",
        "print('TRG 단어 집합의 크기 : {}'.format(len(TRG.vocab)))\r\n",
        "\r\n",
        "# 생성된 단어 집합 내의 단어들은 .stoi를 통해서 확인 가능\r\n",
        "print(TRG.vocab.stoi['his'])\r\n",
        "# dict 형태로 저장되어 있기 때문에 키 값을 넣어 해당 단어의 인덱스 값을 찾을 수 있음\r\n",
        "\r\n",
        "test = {k:v for k,v in TRG.vocab.stoi.items()}\r\n",
        "test.get('mother')  # 496는 빈도인지, 인덱스인지 확인필요\r\n",
        "    "
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SRC 단어 집합의 크기 : 7855\n",
            "TRG 단어 집합의 크기 : 5893\n",
            "27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "496"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTE4btBgH9r-"
      },
      "source": [
        "# 문장의 안의 단어의 순서를 유지하여 입력되어야 함\r\n",
        "# 이를 위해서는 각 배치에 포함되는 단어의 갯수를 맞춰주면 좋은데, BucketIterator를 사용 함\r\n",
        "# BucketIterator는 비슷한 길이를 갖는 데이터를 함께 묶는(batch) Iterator를 정의함. \r\n",
        "# 매 새로운 epoch에서 랜덤한 batch를 생성하는 과정에서 padding을 최소화하기 위해 사용\r\n",
        "\r\n",
        "# batch size란 sample데이터 중 한번에 네트워크에 넘겨주는 데이터의 수를 말한다.\r\n",
        "# 가중치와 편향을 수정하는 간격이라고도 함\r\n",
        "# 배치 사이즈는 GPU RAM에 맞는 크기로 지정하는 것이 좋음\r\n",
        "# 여기서는 논문과 동일하게 128로 하겠음\r\n",
        "\r\n",
        "import torch\r\n",
        "from torchtext.data import BucketIterator\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "# gpu 사용 가능할시 gpu 사용하게끔 cuda로 올려줌\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_dataset, valid_dataset, test_dataset),\r\n",
        "    batch_size=batch_size, device=device\r\n",
        ")"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDZHemqQIBGL",
        "outputId": "c1f2674a-be81-460e-97b8-95ed4840a41d"
      },
      "source": [
        "for i, batch in enumerate(train_iterator):\r\n",
        "    src = batch.src\r\n",
        "    trg = batch.trg\r\n",
        "\r\n",
        "    print('첫번째 배치 크기 : ', {src.shape})\r\n",
        "\r\n",
        "    # 배치 1번에 포함된 문장 정보 출력\r\n",
        "    for i in range(src.shape[1]):\r\n",
        "        print(f\"인덱스 {i}: {src[0][i].item()}\") # 여기에서는 [Seq_num, Seq_len]\r\n",
        "    \r\n",
        "    # 첫번째 배치만 출력되고 중단\r\n",
        "    break"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "첫번째 배치 크기 :  {torch.Size([128, 27])}\n",
            "인덱스 0: 2\n",
            "인덱스 1: 5\n",
            "인덱스 2: 13\n",
            "인덱스 3: 7\n",
            "인덱스 4: 615\n",
            "인덱스 5: 255\n",
            "인덱스 6: 37\n",
            "인덱스 7: 8\n",
            "인덱스 8: 1247\n",
            "인덱스 9: 4\n",
            "인덱스 10: 3\n",
            "인덱스 11: 1\n",
            "인덱스 12: 1\n",
            "인덱스 13: 1\n",
            "인덱스 14: 1\n",
            "인덱스 15: 1\n",
            "인덱스 16: 1\n",
            "인덱스 17: 1\n",
            "인덱스 18: 1\n",
            "인덱스 19: 1\n",
            "인덱스 20: 1\n",
            "인덱스 21: 1\n",
            "인덱스 22: 1\n",
            "인덱스 23: 1\n",
            "인덱스 24: 1\n",
            "인덱스 25: 1\n",
            "인덱스 26: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBoUnA_6IGBO"
      },
      "source": [
        "## 하이퍼 파라미터 설정 및 모델 초기화\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFffo2sIID1H"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "HIDDEN_DIM = 256\r\n",
        "# ==인코더/디코더 layer===================================\r\n",
        "ENC_LAYERS = 3\r\n",
        "DEC_LAYERS = 3\r\n",
        "# ===인코더/디코더 헤드==================================\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "# ==인코더/디코더 feedforward 차원===================================\r\n",
        "ENC_PF_DIM = 512\r\n",
        "DEC_PF_DIM = 512\r\n",
        "# ==인코더/디코더 드롭아웃 비율===================================\r\n",
        "ENC_DROPOUT = 0.1\r\n",
        "DEC_DROPOUT = 0.1"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNR6VjIHIJMl"
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\r\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "# 생성된 단어 집합 내의 단어들은 .stoi를 통해서 확인 가능\r\n",
        "# stoi는 문자를 정수로 바꿔주는 함수 \r\n",
        "\r\n",
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\r\n",
        "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\r\n",
        "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\r\n",
        "\r\n",
        "# 모델 트랜스포머로 설정 \r\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asm_VvBbsKtT"
      },
      "source": [
        "- 모델 가중치 파라미터 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6Q4V1L8Nu5R",
        "outputId": "5ebc3152-6b78-494f-c50c-07f646a503cb"
      },
      "source": [
        "# 모델의 파라미터 수를 확인한다.\r\n",
        "\r\n",
        "def count_parametrs(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad) \r\n",
        "    # numel()은 input텐서의 총 요소 수를 반환한다. \r\n",
        "    # 텐서를 생성하고 requires_grad 속성을 True 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기 시작한다. \r\n",
        "print(f'The model has {count_parametrs(model):,} trainable parameters')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 9,038,853 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvl1vsjPuCQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18614856-a52f-4e86-cc2c-f18f3ad1acc8"
      },
      "source": [
        "def initialize_weights(m):\r\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(m.weight.data)\r\n",
        "    # hasattr(object, name)는 object의 속성(attribute) 존재를 확인\r\n",
        "    # 만약 argument로 넘겨준 object 에 name 의 속성이 존재하면 True, 아니면 False를 반환\r\n",
        "    # m이 weight 속성을 가지고 있고 그 weight의 차원이 1보다 큰 경우,\r\n",
        "    # 균일 분포(uniform distribution)로 가중치를 초기화 한다. \r\n",
        "\r\n",
        "model.apply(initialize_weights)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(7855, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(5893, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fchH3Y60K4G"
      },
      "source": [
        "- 학습 및 평가 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6wJWWTf0Kit"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "# Adam optimizer로 학습 최적화\r\n",
        "LEARNING_RATE = 0.0005\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index= TRG_PAD_IDX)\r\n",
        "# 패딩(padding)에 대해서는 값 무시\r\n",
        "# orch.nn.CrossEntropyLoss는 nn.LogSoftmax와 nn.NLLLoss의 연산의 조합\r\n",
        "# nn.LogSoftmax는 신경망 말단의 결과 값들을 확률개념으로 해석하기 위한 Softmax 함수의 결과에 log 값을 취한 연산이고, \r\n",
        "# nn.NLLLoss는 cross-entropy 손실을 구하는 함수이다.\r\n",
        "# 만일 nn.NLLLoss만 쓴 경우에는 모델 마지막 레이어에 Softmax를 사용하게 된다.\r\n",
        "# 다시말하면, CrossEntropyLoss는 SoftMax를 적용하고 손실 값을 구하는 함수있다. "
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIAukc6LvtCC"
      },
      "source": [
        "# 모델 학습(train) 함수\r\n",
        "\r\n",
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    model.train() # 학습 모드\r\n",
        "    epoch_loss = 0\r\n",
        "\r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        src = batch.src\r\n",
        "        trg = batch.trg\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\r\n",
        "        # 입력을 할 때는 <sos>부터 시작하도록 처리\r\n",
        "        # 트랜스포머 모델은 output과 attention을 리턴함\r\n",
        "        # output: [배치 크기, trg_len - 1, output_dim]\r\n",
        "        # trg: [배치 크기, trg_len]\r\n",
        "\r\n",
        "        output_dim = output.shape[-1]\r\n",
        "\r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        # contiguous()로 새로운 메모리 공간에 데이터를 복사하여 주소값 연속성을 가변적이게 만들어 주고,\r\n",
        "        # view() 로 텐서의 모양을 조절한다.\r\n",
        "        \r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "        # 출력 단어의 인덱스 0(<sos>)은 제외\r\n",
        "\r\n",
        "        # output: [배치 크기 * trg_len - 1, output_dim]\r\n",
        "        # trg: [배치 크기 * trg len - 1]\r\n",
        "\r\n",
        "        loss = criterion(output, trg)\r\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\r\n",
        "        loss.backward() # 기울기(gradient) 계산\r\n",
        "        # forward 함수는 입력 Tensor로부터 출력 Tensor를 계산합니다. backward 함수는 어떤 스칼라 값에 대한 출력 Tensor의 변화도를 전달받고, 동일한 스칼라 값에 대한 입력 Tensor의 변화도를 계산한다.\r\n",
        "        # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를 계산한다.\r\n",
        "        # 내부적으로 각 Module의 매개변수는 requires_grad=True 일 때, \r\n",
        "        # Tensor 내에 저장되므로, 이 호출은 모든 모델의 모든 학습 가능한 매개변수의 변화도를 계산하게 된다.\r\n",
        "\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        # 매 time-step마다 파라미터에 기울기가 더해지므로, 출력의 길이에 따라 기울기의 크기가 달라진다. \r\n",
        "        # 즉, 길이가 길수록 자칫 기울기가 너무 커질 수 있으므로, 학습률을 조절하여 경사하강법의 업데이트 속도를 조절해야 한다.\r\n",
        "        # 너무 큰 학습률을 사용하면 (gradient의 크기인 norm이 너무 큰 경우) \r\n",
        "        # 경사하강법에서 한 번의 업데이트 스텝의 크기가 너무 커져, 자칫 잘못된 방향으로 학습 및 발산해버릴 수 있기 때문이다.\r\n",
        "        # 이때 그래디언트 클리핑gradient clipping이 도움이 된다. \r\n",
        "        # clip_grad_norm_()는 모든 기울기를 함께 스케일(scale) 하는 함수이다. \r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "        # Optimizer의 step 함수를 호출하면 매개변수가 갱신된다.\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        # loss.item()은 loss의 스칼라 값이다.\r\n",
        "        # 전체 손실 값 계산\r\n",
        "\r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBvNFxxP6ZY9"
      },
      "source": [
        "# 모델 평가(evaluate) 함수\r\n",
        "\r\n",
        "def evaluate(model, iterator, criterion):\r\n",
        "    model.eval() # 평가모드\r\n",
        "    epoch_loss = 0\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "            src = batch.src\r\n",
        "            trg = batch.trg\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "            loss = criterion(output, trg)\r\n",
        "            epoch_loss += loss.item()\r\n",
        "\r\n",
        "    return epoch_loss / len(iterator)    \r\n",
        "        "
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhGROlMgr46T"
      },
      "source": [
        "- 학습 및 검증 진행 \r\n",
        "    * 학습 횟수(epoch) : 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXLWSC7Or0bN"
      },
      "source": [
        "# 학습 경과 시간 확인 함수\r\n",
        "\r\n",
        "import math\r\n",
        "import time\r\n",
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time/60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins*60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "kcxBkS2zsyBw",
        "outputId": "e425c358-58df-4233-85b4-9208eea3d2a7"
      },
      "source": [
        "import time\r\n",
        "import math\r\n",
        "import random\r\n",
        "\r\n",
        "N_EPOCHS = 10 # 10 에포크 학습\r\n",
        "CLIP = 1\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    start_time = time.time #시작 시작 기록\r\n",
        "\r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "\r\n",
        "    end_time = time.time # 종료 시간 기록\r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss  # 학습하면서 가장 작은 손실 평가를 갱신함\r\n",
        "        torch.save(model.state_dict(), 'transformer_german_to_english.pt')\r\n",
        "        # state_dict는 모델 parameters를 Tensor로 매핑한 Python dict 객체\r\n",
        "\r\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "                # 정수 2자리로 표현\r\n",
        "    print(f'\\tTrain Loss : {train_loss:.4f} | Train PPL: {math.exp(train_loss):.4f}')\r\n",
        "           # \\t는 탭 들여쓰기             # 실수 소수점 4자리까지 표현\r\n",
        "        #    Math.exp()함수는 x를 인수로 하는 e^x 값을 반환, 여기서는 e는 Euler 의 상수 e (약 2.71828), e^train_loss의 형태\r\n",
        "    print(f'\\tValidation Loss : {valid_loss:.4f} | Validation PPL: {math.exp(valid_loss):.4f}')\r\n",
        "    # 펄플렉서티(perplexity, PPL)는 언어 모델을 평가하기 위한 내부 평가(Intrinsic evaluation) 지표이다. '낮을수록' 언어 모델의 성능이 좋다는 것을 의미한다.\r\n",
        "    # 언어 모델의 PPL이 10이 나왔다면, 해당 언어 모델은 테스트 데이터에 대해서 다음 단어를 예측하는 모든 시점(time-step)마다 \r\n",
        "    # 평균적으로 10개의 단어를 가지고 어떤 것이 정답인지 고민하고 있다고 볼 수 있다."
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-a48a4dd8d945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;31m#시작 시작 기록\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-b37ed19fda74>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# 출력 단어의 마지막 인덱스(<eos>)는 제외\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# 입력을 할 때는 <sos>부터 시작하도록 처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-e4c149e26d60>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# trg_mask: [batch_size, 1, trg_len, trg_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0menc_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m# enc_src: [batch_size, src_len, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-6e1ad98b40e2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# src: [batch_size, src_len, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-3db6bca1a9e7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 인코더 self-attention layer는\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0m_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 필요한 경우 self attention에서 마크스 행렬을 이용하여 어텐션할 단어를 조절 가능\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# src-src 이거나, src-src_masked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-9753ab86f12c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# query의 모양은 [batch_size, query_len, hidden_dim] 형태, 여기서 인덱스 0번 batch_size을 받음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "-t5pHFDav82d",
        "outputId": "6efff06a-588b-4e3d-b1f2-80f8b3cc0c44"
      },
      "source": [
        "# 학습된 모델 저장\r\n",
        "from google.colab import files\r\n",
        "\r\n",
        "files.download('transformer_german_to_english.pt')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-b217b57a6dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformer_german_to_english.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: transformer_german_to_english.pt"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDfko5vI2bpt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}